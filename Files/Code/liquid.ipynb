{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "04557384",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: transformers in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.57.1)\n",
                        "Requirement already satisfied: filelock in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (3.20.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.35.3)\n",
                        "Requirement already satisfied: numpy>=1.17 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2.3.3)\n",
                        "Requirement already satisfied: packaging>=20.0 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (6.0.3)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2025.9.18)\n",
                        "Requirement already satisfied: requests in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (2.32.5)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (0.6.2)\n",
                        "Requirement already satisfied: tqdm>=4.27 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
                        "Requirement already satisfied: colorama in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.4.3)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (3.10)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joaov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests->transformers) (2025.8.3)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "\"\"\" Installing library \"\"\"\n",
                "\n",
                "%pip install -U transformers\n",
                "# %pip install transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "10b21b43",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
                    ]
                },
                {
                    "ename": "ImportError",
                    "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[32m      4\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mLiquidAI/LFM2-350M-Extract\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mLiquidAI/LFM2-350M-Extract\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m messages = [\n\u001b[32m      7\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWho are you?\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      8\u001b[39m ]\n\u001b[32m      9\u001b[39m inputs = tokenizer.apply_chat_template(\n\u001b[32m     10\u001b[39m \tmessages,\n\u001b[32m     11\u001b[39m \tadd_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \treturn_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m ).to(model.device)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2157\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   2155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m2157\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2143\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   2140\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   2142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m2143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
                        "\u001b[31mImportError\u001b[39m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
                    ]
                }
            ],
            "source": [
                "# Load model directly\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-350M-Extract\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\"LiquidAI/LFM2-350M-Extract\")\n",
                "messages = [\n",
                "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
                "]\n",
                "inputs = tokenizer.apply_chat_template(\n",
                "\tmessages,\n",
                "\tadd_generation_prompt=True,\n",
                "\ttokenize=True,\n",
                "\treturn_dict=True,\n",
                "\treturn_tensors=\"pt\",\n",
                ").to(model.device)\n",
                "\n",
                "outputs = model.generate(**inputs, max_new_tokens=40)\n",
                "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "036a40e0",
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'torch' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n",
                        "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n",
                        "\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
                        "\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLiquidAI/LFM2-350M-Extract\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "\u001b[32m      5\u001b[39m messages = [\n",
                        "\u001b[32m      6\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWho are you?\u001b[39m\u001b[33m\"\u001b[39m},\n",
                        "\u001b[32m      7\u001b[39m ]\n",
                        "\u001b[32m      8\u001b[39m pipe(messages)\n",
                        "\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\pipelines\\__init__.py:1017\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n",
                        "\u001b[32m   1012\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
                        "\u001b[32m   1013\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mYou cannot use both `pipeline(... dtype=..., model_kwargs=\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:...})` as those\u001b[39m\u001b[33m'\u001b[39m\n",
                        "\u001b[32m   1014\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m arguments might conflict, use only one.)\u001b[39m\u001b[33m\"\u001b[39m\n",
                        "\u001b[32m   1015\u001b[39m         )\n",
                        "\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtorch\u001b[49m, dtype):\n",
                        "\u001b[32m   1018\u001b[39m         dtype = \u001b[38;5;28mgetattr\u001b[39m(torch, dtype)\n",
                        "\u001b[32m   1019\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m] = dtype\n",
                        "\n",
                        "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
                    ]
                }
            ],
            "source": [
                "# Use a pipeline as a high-level helper\n",
                "from transformers import pipeline\n",
                "\n",
                "pipe = pipeline(\"text-generation\", model=\"LiquidAI/LFM2-350M-Extract\")\n",
                "messages = [\n",
                "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
                "]\n",
                "pipe(messages)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0ebab853",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
                        "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
                    ]
                }
            ],
            "source": [
                "\"\"\" Importing libraries \"\"\"\n",
                "\n",
                "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9eb631a0",
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'torch' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m liquid_nano = \u001b[33m'\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mLiquidAI/LFM2-350M-Extract\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliquid_nano\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m      8\u001b[39m model = AutoModelForCausalLM.from_pretrained(model_name)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\pipelines\\__init__.py:1017\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1012\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1013\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mYou cannot use both `pipeline(... dtype=..., model_kwargs=\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:...})` as those\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1014\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m arguments might conflict, use only one.)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1015\u001b[39m         )\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtorch\u001b[49m, dtype):\n\u001b[32m   1018\u001b[39m         dtype = \u001b[38;5;28mgetattr\u001b[39m(torch, dtype)\n\u001b[32m   1019\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m] = dtype\n",
                        "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
                    ]
                }
            ],
            "source": [
                "\"\"\" Use a pipeline as a high-level helper \"\"\"\n",
                "\n",
                "liquid_nano = 'text-generation'\n",
                "model_name = 'LiquidAI/LFM2-350M-Extract'\n",
                "\n",
                "pipe = pipeline(liquid_nano, model=model_name)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1a4f139",
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\" Load model directly \"\"\"\n",
                "\n",
                "product = 'APG STO0045  Camping Stove Portable Cooking Equipment Welding BBQ Butane Hiking Camping Gas Burners Gas Adapter Torch Lighter'\n",
                "messages = [\n",
                "    {\"role\": \"user\", \"content\": product},\n",
                "]\n",
                "inputs = tokenizer.apply_chat_template(\n",
                "\tmessages,\n",
                "\tadd_generation_prompt=True,\n",
                "\ttokenize=True,\n",
                "\treturn_dict=True,\n",
                "\treturn_tensors=\"pt\",\n",
                ").to(model.device)\n",
                "\n",
                "outputs = model.generate(**inputs, max_new_tokens=40)\n",
                "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a66121b1",
            "metadata": {},
            "source": [
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
